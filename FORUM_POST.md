# Mojo BenchSuite: TestSuite-style patterns for benchmarking ðŸ”¥

I just discovered Mojo's stdlib `benchmark` module and wanted to share a complementary approach inspired by TestSuite.

**Repo**: https://github.com/DataBooth/mojo-benchsuite

Benchmarking is clearly important to the Modular team (the stdlib module is excellent!). This project explores making it as frictionless as TestSuite.

## Key additions over stdlib `benchmark`:

ðŸŽ¯ **Suite-level organisation** â€” Group and run multiple benchmarks  
ðŸ“Š **Environment capture** â€” OS/CPU/version for reproducibility  
ðŸ”„ **Adaptive iterations** â€” Auto-adjust for reliable statistics  
ðŸ’¾ **Multiple outputs** â€” Console, markdown, CSV with timestamps

## Example

```mojo
from benchsuite import BenchReport

fn my_algorithm():
    pass

def main():
    var report = BenchReport()
    report.benchmark[my_algorithm]("my_algorithm")
```

**Future**: Exploring `TestSuite.discover_tests[__functions_in_module__()]()` pattern for auto-discovery of `bench_*` functions.

Feedback and ideas welcome!
